---
title: prolfqua - An R-package for Analysing Proteomics Label Free Quantification Experiments
author: 
  - Witold E. Wolski^[[Functional Genomics Center Zurich (FGCZ)](https://fgcz.ch) - Swiss Federal Institute of Technology in Zurich \| University of Zurich]
  - Paolo Nanni$^*$
  - Jonas Grossmann$^*$
  - Maria d'Errico$^*$
  - Ralph Schlapbach$^*$
  - Christian Panse$^*$
output: 
  bookdown::pdf_document2:
    toc: true
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: console
abstract: |
  Protein mass spectrometry is widely used for quantitative proteomics
  studies and in particular for relative protein quantification.
  Nevertheless, there is still a need for a flexible and easy-to-use, 
  transparently supporting a variety of well principled statistical
  inference procedures data analysis application programmer interface
  in R. The `r BiocStyle::Githubpkg("fgcz/prolfqua")` R package integrates
  essential steps of the mass spectrometry-based label-free quantification
  data analysis workflow as: quality control, data normalization, protein
  aggregation, statistical modeling, hypothesis testing, and sample size
  estimation.
  We use `r BiocStyle::Githubpkg("fgcz/prolfqua")` to visualize and model
  simple experimental designs with a single explanatory variable and complex
  experiments with multiple factors.
  Furthermore, we use `r BiocStyle::Githubpkg("fgcz/prolfqua")` to benchmark
  data acquisition, data preprocessing or data modeling methods. Furthermore,
  we apply `r BiocStyle::Githubpkg("fgcz/prolfqua")` to develop highly
  customizable, visually appealing, and interactive data analysis reports
  in PDF or HTML format.
  The R package is available through https://github.com/fgcz/prolfqua.
---


```{r setup, include=FALSE}
## See ACS guidelines [https://publish.acs.org/publish/author_guidelines?coden=ancham#manuscript_text_components]
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 7, fig.height = 5)
```

\newpage

# Introduction

Proteins are carriers of function and structure in living cells. Hence, measuring changes in protein abundance is the subject of active research [@vidova2017review]. However, identifying and quantifying intact proteins in complex samples containing hundreds of proteins is not possible using existing methods at the moment. Hence, the bottom-up mass spectrometric method is employed [@Bubis2017, @da2020philosopher]. Proteins that are extracted from biological samples are specifically and reproducible digested into protein fragments - peptides. The peptides are first separated by their chemical and physical properties using High-Pressure Liquid Chromatography (HPLC). Afterward, they are weighted, identified, and quantified using mass spectrometric techniques, e.g., Electro-Spray-Ionization Mass Spectrometry (ESI-MS). Finally, peptide identification is achieved by fragmenting and matching the measured fragment masses to theoretical masses computed from known peptide sequences[@eng2015deeper, @yu2016pipi, @kong2017msfragger]. For quantification, intact peptide ions [@yu2020fast, @Cox2008MaxQuant] or products of peptide ion fragmentation [@rost2014openswath, @demichev2020dia] are counted and aggregated. Finally, the identified and quantified peptides are assigned to proteins based on protein sequence information.

Label-Free Quantification (LFQ) Proteomics Experiments enable monitoring relative abundances of thousands of proteins in biological samples. For most studies, parallel group designs are used, e.g., treatment versus control are used [@de2022apoe2, @laubscher2021baf]. More recently, more complex experimental designs are implemented with increasing numbers of samples, i.e., factorial designs and longitudinal studies (time series), sometimes with repeated measurements on the same subject [@tan2022proteomic, @meier2021reduced]. The obtained data can be modeled using linear fixed-, mixed-, or random-effects models [@Bates2015FittingJSS]. Based on these models, tests can be applied to examine whether a factor or interactions are significant, or differences between factor levels can be estimated, and their significance tested. Proteins can be ordered or grouped using the estimated fold-change, $t$-statistics, scaled $p$-value, or false discovery rate (FDR) and then subjected to gene set enrichment or over-representation analysis [TODO ...] or validation experiments.

Several packages exist to model mass spectrometry LFQ experiments, e.g., `r BiocStyle::Biocpkg("limma")` [@Ritchie2015], `r BiocStyle::Biocpkg("MSstats")` [@Choi2014], `r BiocStyle::Biocpkg("PECA")` [@Suomi2017bEnhanced],  `r BiocStyle::Biocpkg("msqrob2")` [@Goeminne2016] or `r BiocStyle::Biocpkg("proDA")` [@bioxrvproDA2020], to mention some, all implemented in the R. Each of them has some unique features. For example, `r BiocStyle::Biocpkg("MSstats")` infers the model from the sample annotation and generates the model formula. At the same time, `r BiocStyle::Biocpkg("limma")` allows to specify a linear model formula and implements the empirical Bayes variance shrinkage method. The R-package `r BiocStyle::Biocpkg("PECA")`, performs a roll-up of peptide level fold change and $p$-value estimates, obtained from `r BiocStyle::Biocpkg("limma")`, to protein level estimates. Furthermore, `r BiocStyle::Biocpkg("msqrob2")` implements two models: robust linear models fitted to protein intensities and robust ridge regression fitted to peptide intensities and combines them with empirical Bayes variance shrinkage known from `r BiocStyle::Biocpkg("limma")`. The `r BiocStyle::Biocpkg("proDA")` package implements a linear probabilistic dropout model to account for missing data without imputation.

When analyzing parallel group designs using a single explanatory variable, we can use all packages for the statistical data analysis of the data; however, if we want to model factorial designs or repeated measurements, we can use only some of them. Table \@ref(tab:tableOverview) gives an overview of the models and features supported by these packages.

```{r tableOverview}
xx <- list(
  triqler = c("pd" = "Y"),
  MSstats = c("pd" = "Y", "rm" = "Y", "mem" = "y"),
  ROPECA = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y", eb = "Y"),
  limma  = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y",eb = "Y"),
  MSqRob2_rlm = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y",eb = "Y", md = "?"),
  proDA = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y",eb = "Y", md = "Y"),
  prolfqua = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y", mem = "Y",eb = "Y", md = "Y"))

bb <- data.frame(dplyr::bind_rows(xx))
rownames(bb) <- names(xx)
knitr::kable(data.frame(bb), caption = "Rows - R package, Columns - types of models supported: pd - parallel design, rm - repeated measurements, fd - factorial design, int - interactions among factors, mem - mixed effect models, eb - empirical Bayes, md - missing data modelling. Y - yes, y - yes but only for random effects, * - repeated measurements are only modeled using a fixed effects. ? - the hurdle model is not implemented in the msqrob2 package yet.")
```

We were inspired by the R package `r BiocStyle::CRANpkg("caret")` [@caret2008], which enables to call various machine learning (ML) methods, making selecting the best ML algorithm for your problem easy. We aimed for a similar R package for the analysis of LFQ data. However, after examining the R packages for modeling proteomics quantification data, we observed that model specification, input, and output formats differ widely and wildly. However, all of them fit some type of linear model either to peptide or protein intensities, determine contrasts among conditions, and afterward, some of them apply empirical Bayes variance shrinkage. Therefore, the revised goal was to provide a modular object oriented (OO) design, with R linear models as a core, where features such as $p$-value aggregation, e.g., `r BiocStyle::Biocpkg("PECA")` or variance shrinkage could be easily enabled (see Figure \@ref(fig:ContrastUML)).

Models in `r BiocStyle::Githubpkg("fgcz/prolfqua")` are specified using R’s linear model and mixed model formula interface. Therefore, knowledge of the R regression model infrastructure [@faraway2016extending,@MASS2002] is an advantage when using our package. Furthermore, this glass box approach should make it easy to reimplement an analysis which was performed with `r BiocStyle::Githubpkg("fgcz/prolfqua")` using base R or other programming languages by reading the analysis script. However, acknowledging the R formula interface’s complexity to non-statisticians and the popularity of `r BiocStyle::Biocpkg("MSstats")`, we provide the functionality to suggest a model formula from the sample annotation provided in tabular form similarily to `r BiocStyle::Biocpkg("MSstats")` [@Choi2014].

An important aspect of mass spectrometric data are missing values. Missing values lead to groups of varying size. Linear and mixed effect models can handle unbalanced designs, i.e. different group sizes. As long as there is at least one observation per group they do produce unbiased estimates. We use this feature instead of imputing missing observations. 
However, if there are no observation in one of the groups, then contrasts involving this group can not be estimated. To address this, for the group without observations, we use a point prior equal to the average of $3\%$ smallest group intensities means in the dataset and infer the variance using the pooled the variance of other groups. Proteins for which the fold change was imputed in such a way are explicitly annotated in the result table.

Other means of modelling missing information in proteomics data using the Hurdle model is discussed by [@goeminne2020msqrob], while the R package `r BiocStyle::Biocpkg("proDA")` models missingness using probabilistic dropout models [@bioxrvproDA2020]. 

Other means of modelling missing information in proteomics data using the Hurdle model is discussed by [@goeminne2020msqrob], while the R package `r BiocStyle::Biocpkg("proDA")` models missingness using probabilistic dropout models [@bioxrvproDA2020]. 

Furthermore, the functionality of `r BiocStyle::Githubpkg("fgcz/prolfqua")` also includes methods specific to proteomics data. For example, we implemented strategies to estimate protein intensities from peptide intensities: top N, Tukey's median polish, robust linear models [@Grossmann2010, @goeminne2020msqrob]. Furthermore, peptide or protein intensities can be scaled using the robust scaling method or transformed using _quantile_ normalization or `r BiocStyle::Biocpkg("vsa")` to remove systematic differences among samples and heteroscedasticity. In this respect, `r BiocStyle::Githubpkg("fgcz/prolfqua")` can be compared with other R packages such `r BiocStyle::Biocpkg("DEP")` [@DEP2018] or `r BiocStyle::Biocpkg("POMA")` [@POMA2021] which support the entire LFQ analysis pipeline. However, we also implemented functionality to streamline the benchmarking of the implemented methods. We use the Ionstar [@shen2018ionstar] dataset to benchmark the analysis pipeline and modeling methods implemented within `r BiocStyle::Githubpkg("fgcz/prolfqua")` and to compare our results with those of `r BiocStyle::Biocpkg("MSstats")` and `r BiocStyle::Biocpkg("proDA")`.

# Methods

## Implementation

We store all the data needed for analysis in a single data frame in a tidy table, i.e., every column is a variable, every row is an observation, every cell is a single value[@tidydata2014]. Using an __R6__[@R6cite] configuration object, we specify what variable is in which column, making it easy to integrate new inputs in `r BiocStyle::Githubpkg("fgcz/prolfqua")` if provided as a tidy table. For example, to visualize tidy _Spectronaut_, or _Skyline_ outputs, or data in `r BiocStyle::Biocpkg("MSstats")` format, only a few lines of code are needed to update the `r BiocStyle::Githubpkg("fgcz/prolfqua")` configuration. For popular software like _MaxQuant_ or _MSFragger_, which stores the same variable, e.g., intensity, in multiple columns, one for each sample, we implemented methods that transform the data into tidy tables. Relying on the tidy data table enable us to easily interface with many data manipulation, visualization, and modeling methods implemented in base __R__[@RCoreTeam2021] and the tidyverse[@tidyverse2019]. We use __R6__ classes to structure the functionality of the package (see Figure \@ref(fig:LFQData) and Figure \@ref(fig:ContrastUML)). __R6__ classes are well supported, e.g. by auto-completion features, and easy to use (OOP).

(ref:LFQData) Class Diagram for LFQData and related classes. The LFQData_Plotter class uses the LFQData class to implement methods for plotting. Similarily the LFQData_Stats, LFQData_Summary, reference the LFQData class and group methods for variance and sample size estimation or for summarizing peptide and protein counts.

```{r LFQData, echo=FALSE, fig.cap="(ref:LFQData)", out.width = '90%'}
knitr::include_graphics("LFQDataUML.png")
```

__R__ linear model and linear mixed effect models allow modeling parallel designs, repeated measurements, factorial designs, and many more. __R__'s formula interface for linear models is flexible, widely used, and well documented ([@faraway2016extending], page : ). This makes it easy to reproduce an analysis performed with `r BiocStyle::Githubpkg("fgcz/prolfqua")` in other statistical programming languages. We implemented features specific to high throughput experiments, such as the experimental Bayes variance and $p$-value moderation, which utilizes the parallel structure of the protein measurements and the analysis [@Ritchie2015]. We also compute probabilities of differential protein regulation based on peptide level models [@Suomi2017bEnhanced]. We use R6 to model the statistical modeling functionality in `r BiocStyle::Biocpkg("prolfqua")` (see Figure \@ref(fig:ContrastUML)).

(ref:ContrastUML) UML diagram of modeling and contrast related classes. Different strategies, e.g., _lm_, _lmer_, and _glm_ can be used to fit various types of models using a model builder (see code). All classes computing contrasts implement the _Contrast_ interface.

```{r ContrastUML, echo=FALSE, fig.cap="(ref:ContrastUML)", out.width = '90%'}
#fig_svg <- cowplot::ggdraw() + cowplot::draw_image("ContrastClassesUML.svn")
#plot(fig_svg)

knitr::include_graphics("ContrastClassesUML.png")
```


## Dataset

TODO(wew): Ionstar dataset, give Pride ids, describe MaxQuant settings and version. Give reference to Experiment hub and the _prolfquaData_ package.

## Modelling

### Estimating contrasts for linear models

Given a linear model contrasts can be computed by $\hat{\beta_{c}} = \sum l\beta_{m}$ and $\textrm{var} \hat\beta_c = l\sigma^2 (X^T X)^{-1} l^T$, with $X$ being the design matrix, $\beta_m$ the model parameters and $l$ an array of coefficients. The degrees of freedom for the contrast are equal to the residual degrees of freedom of the linear model.

For estimating contrasts from mixed effects models we used the function `contest` implemented in the R package `r BiocStyle::CRANpkg("lmerTest")` [@Kuznetsova2017lmerTest] and used the Satterthwaite method to estimate the denominator degrees of freedom. These methods are implemented in the class `Contrast` (see Figure \@ref(fig:ContrastUML))

### Contrast estimation in presence of missing data.

If there are no observation in a group, we assume that there are no observations because of the limit of detection (LOD). Furthermore, we assume that the variance of the protein is constant in all the conditions.
We impute the mean estimate of the group by using the average of $1\%$ smallest group mean intensities in the experiment. To estimate the variance in the condition, we use the pooled variance based on all conditions: 

\begin{equation} 
s_p^2=\frac{\sum_{i=1}^k (n_i - 1)s_i^2}{\sum_{i=1}^k (n_i - 1)}
\end{equation} 

with $n_i$ the number of observations, and $s_i$ the standard deviation in each condition.
The standard deviation for the $t$-statistics is then given by:

\begin{equation} 
s = \sqrt{\frac{2n_g s_p^2}{n}},
\end{equation} 

Where, $n_g$ is the number of groups and $n$ is the number of observations.

This methods are implemented in the class `ContrastSimpleImpute` (see Figure \@ref(fig:ContrastUML)).

### $p$-value moderation

From the linear and the mixed effect models, we can obtain the residual standard deviation $\sigma$, and degrees of freedom $df$. [@Smyth2004linear] discuss how, using the empirical Bayes paradigm, to use the $\sigma$ and $df$ of all models to estimate a prior $\sigma$ and prior $df$, and posterior $\tilde \sigma$. These can be used to moderate the $t$-statistics by:

\begin{equation} 
\tilde{t}_{pj} = \frac{t_{pj} s_p}{\tilde{s}_p}
\end{equation} 

and subsequently the $p$-values. We implemented this method in the class `ContrastModerated` (Figure \@ref(fig:ContrastUML)).

### Summarizing peptide level models

To summarize peptide level models to protein models, we applied the method suggested by [@Suomi2017bEnhanced]  to use the median scaled $p$-value of the peptide models and cumulative distribution function of the beta distribution function to determine a regulation probability of the protein.

To obtain the median $p$-value of a protein, we first rescaled the peptide $p$-values by taking the direction of the fold-change $\hat \beta$ into account, i.e.:

\begin{equation}
p_{s} =
  \begin{cases}
1-p, & \textrm{if}~ \hat{\beta} > 0\\
p-1, & \textrm{otherwise}
\end{cases}
(\#eq:scalepvalue)
\end{equation}

Afterwards, the median scaled $p$-value $\tilde{p}_s$ is determined and using the transformation below, transformed back onto the original scale. 

\begin{equation}
\tilde{p} = 1 - |\tilde{p}_{s}|
\end{equation}

Because we used the median as the i-th order statistic $i = \frac{n}{2} + 0.5$. Therefore, $\gamma = i = \frac{n}{2} + 0.5$ and $\delta = n - i + 1 = n - (\frac{n}{2} + 0.5) + 1 = \frac{n}{2} + 0.5 = \gamma$ are used to parameterize the cumulative distribution function (CDF) of the Beta distribution. We implemented this method in the class `ContrastROPECA` (Figure \@ref(fig:ContrastUML)).

## Usage

The code snippets in this section demonstrate the basic usage of the `r BiocStyle::Githubpkg("fgcz/prolfqua")` R package. As input data we use Ionstar dataset.
Peptide intensities are $\log_2$ transformed and robust z-score scaled but then rescaled to preserves the original range of the data using the method `robscale`. We show the distribution of the normalized intensities in Figure \@ref(fig:prepro) Panel A. Protein intensities are estimated using Tukey's median polish. Figure \@ref(fig:prepro) Panel B shows the peptide intensities and the estimated protein intensities. The boxplot (Panel D) shows the distribution of the protein intensity estimate in each condition. Finally, Panel C shows the distribution of the protein standard deviations for each condition.

```{r prepro,  echo = TRUE , fig.cap="(ref:scaling)", out.width = '90%', fig.width = 8, fig.height = 8}
## load peptide level data
d <- prolfqua::prolfqua_data('data_ionstar')$filtered()

## create R6 obejct
lfqd <- prolfqua::LFQData$new(d$data, d$config) 

##  transform intensities
t <- lfqd$get_Transformer()

lfqd <-  t$log2()$robscale()$lfq

## plotting
pl <- lfqd$get_Plotter()
p_a <- pl$intensity_distribution_density() +
  ggplot2::labs(tag = "A") + ggplot2::theme(legend.position = "none")

## infer protein intensities from peptide intens
agr <- lfqd$get_Aggregator()
lfqp <- agr$medpolish()

## plotting
p_b <- agr$plot()$plots[[1]] + ggplot2::labs(tag = "B")
p_c <- lfqp$get_Stats()$violin() + ggplot2::labs(tag = "C")
pl <- lfqp$get_Plotter()
p_d <- pl$boxplots()$boxplot[[1]] + ggplot2::labs(tag = "D")
ggpubr::ggarrange(p_a, p_b, p_c, p_d)
```

(ref:scaling) Panel A - Peptide intensity distributions for 20 samples. For each sample a line with a different colour is shown. Panel B - Peptide intensities for protein _5NTD_ are shown using a line of different colour, and the protein intensity estimate is shown using a fat black line, Panel C - distribution of standard deviations of all proteins in each dilution group (a, b, c, d, e) and overall (all), Panel D - Distribution of protein intensities for protein _5NTD_.

The following code example illustrates how we compute contrast comparing the protein intensities in the condition. First, the differences and the linear model are specified. Afterward, the model is fitted to the data using the `build_model` function. Next, we examine the importance of the factors in the model using the ANOVA function, which facilitates model selection for models with one explanatory variable. Finally, we estimate the contrasts either from the linear model using the `Contrasts` class or directly from the data using the `ContrastsSimpleImpute` class. Afterward, we apply $t$-statistic moderation using the `ContrastModerated` class. Finally, the `addContrastResults` function merges both sets of contrast estimates, preferring the one obtained from the linear model. Figure \@ref(fig:exampleContrasts) Panel A shows the distribution of the $p$-values, while figure Panel B shows the volcano-plot for each comparison.

(ref:exampleContrasts) Left panel - histogram of $p$-values. Right panel - volcano plots.

```{r exampleContrasts, fig.cap="(ref:exampleContrasts)", echo=TRUE, out.width = '90%'}
contrasts <- c(
  "dilution_(9/7.5)_1.2" =   "dilution.e - dilution.d",
  "dilution_(7.5/6)_1.25" =   "dilution.d - dilution.c"
)
## fit model
lmmodel <- paste(lfqp$intensity_column()," ~ dilution.")
modelFunction <- prolfqua::strategy_lm( lmmodel, model_name = "lm")
models <- prolfqua::build_model(lfqp, modelFunction)
p0 <- models$anova_histogram()

## compute contrasts from linear model and with imputation
contr <- prolfqua::Contrasts$new(models, contrasts) |> 
  prolfqua::ContrastsModerated$new()

conI <- prolfqua::ContrastsSimpleImpute$new( lfqp, contrasts) |> 
  prolfqua::ContrastsModerated$new()

# TODO(wew): give tmp a meaningful name
tmp <- prolfqua::addContrastResults(prefer = contr, add = conI)

## visualize results
pl <- tmp$merged$get_Plotter()
p1 <- pl$histogram()$p.value
p2 <- pl$volcano()$FDR +
   ggplot2::theme(legend.position = "bottom")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## Benchmarking

The peptide intensities from the _MaxQuant_ _peptide.txt_ file were $log_2$ transformed, and subsequent robust z-score transformation, where median and the mean average deviation (mad) were obtained from the human proteins only. We removed _one hit wonders_, i.e. proteins with a single peptide assignment. No further data filtering was used. We also did not use imputation. The protein level linear model was fitted to the protein intensities inferred from the peptide intensities using the Tukey's median polish. The mixed effect linear model, and the `r BiocStyle::Biocpkg("PECA")` model were fitted to the peptide intensities. The details of the `r BiocStyle::Githubpkg("fgcz/prolfqua")` benchmark implementation are available online  [BenchmarkingIonstarData](https://wolski.github.io/prolfqua/articles/BenchmarkingIonstarData.html).

For benchmarking of `r BiocStyle::Biocpkg("MSstats")` we used the defaults specified in the [Benchmarking_MSstats](https://wolski.github.io/prolfqua/articles/)

The Benchmark functionality of `r BiocStyle::Githubpkg("fgcz/prolfqua")` includes ROC curves and computes partial areas under those curves (pAUC) and other scores, e.g., the false discovery proportion FDP. We use those plots to study how well linear, mixed effect models or $p$-value moderation models quantitative mass spectrometric high throughput experiments and compare them with results produced by `r BiocStyle::Biocpkg("MSstats")` [@MSstats2014] and `r BiocStyle::Biocpkg("proDA")` [@bioxrvproDA2020] (see also Figure \@ref(fig:FDRfdp))

The IonStar [@shen2018ionstar] dataset contains _H. sapiens_ proteins with constant concentrations and _E. coli_ proteins with varying concentrations. We know that for _H. sapiens_ proteins, the difference $\beta$ between two dilutions should be $\beta = 0$. While for _E. coli_ proteins, we know that the difference between dilutions should be $\beta \ne 0$. To compare the performance of the various methods implemented in `r BiocStyle::Githubpkg("fgcz/prolfqua")` we use only the contrasts resulting in small fold-changes $\beta = (1.20, 1.25, 1.30, 1.50)$.

We can use various statistics to examine the alternative hypothesis $\beta \ne 0$: the contrast estimate, i.e. the $\log_2$ fold-change $\beta$, the $t$-statistic $\frac{\beta}{\sqrt{var(\beta)}}$, or the $p$-value and moderated $p$-value. For each statistic and each value of the statistics we then compute a confusion matrix (see Table \@ref(tab:confusionMatrix)). From the confusion matrix we obtain measures such as true positive rate ($TPR$), false positive rate ($FPR$), or false discovery proportion ($FDP$) which are given by:

```{r knitrConfusionMatrix}
table <- data.frame( c("beta != 0", "beta == 0", "Total"),
                     matrix(c("TP", "FP", "R", "FN", "TN", "", "P", "N", "m"),
                            ncol = 3, byrow = T))
colnames(table) <- c("Prediction \\ Truth","E.coli", "H.sapiens", "Total")

knitr::kable(table, caption = "Confusion matrix, TP - true positive, FP - false positive, FN - false negative, P - all positive cases (all E. coli proteins), N - all negative cases (all H. sapiens proteins), m- all proteins.")
```

with

\begin{eqnarray}
TPR &= \frac{TP}{TP+FN} = \frac{TP}{P}\\
FPR &= \frac{FP}{FP+TN} = \frac{FP}{N}\\
FDP &= \frac{FP}{TP + FP} = \frac{FP}{R}
\end{eqnarray}

By plotting the $TPR$ versus the $FPR$ we obtain the receiver operator characteristic curve (ROC curve). The area under the curve (AUC) or partial areas under the curve (pAUC), at various values of the $FPR$, are measures of performance derived from the ROC curve. By using these measures we can compare the performances of the statistics produced by the various methods examined.


```{r readBenchmarkData}
allBenchmarks <- readRDS("../../inst/Benchresults/allBenchmarks.RDS")
benchmark_msstats <- readRDS("../../inst/Benchresults/benchmark_msstats.RDS")
benchmark_prodA <- readRDS("../../inst/Benchresults/benchmark_medpolish_proDA.RDS")
msFragger <- readRDS(file = "../../inst/Benchresults/MSFragger_medpol_benchmark.RDS")

allBenchmarks$benchmark_mssstats <- benchmark_msstats
allBenchmarks$benchmark_msFragger <- msFragger
allBenchmarks$benchmark_proDA <- benchmark_prodA
allBenchmarks <- allBenchmarks[c("benchmark_imputation","benchmark_ProtModerated",  "benchmark_mixedModerated", "benchmark_ropeca","benchmark_merged","benchmark_mssstats","benchmark_proDA"   )]
```

(ref:benchmarkROC)  Left panel - Difference to mean partial area under the ROC curve for various models, at $10\%$ FPR, by fold change.

```{r benchmarkROC, fig.cap="(ref:benchmarkROC)", out.width = '90%', fig.width=12, fig.height=10}
ttt <- sapply(allBenchmarks, function(x){x$complete(TRUE)})

res <- purrr::map_df(allBenchmarks, function(x){x$pAUC()})
res <- res |> dplyr::mutate(whatfix = dplyr::case_when(what == "scaled.beta.based.significance" ~ "scaled.p.value", TRUE ~ what))

norm <- res |> dplyr::group_by(contrast,whatfix) |> dplyr::summarize(meanpAUC_10 = mean(pAUC_10))
res <- dplyr::inner_join(res, norm)
res <- dplyr::mutate(res , pAUC_10n = pAUC_10 - meanpAUC_10)

resAllB <- res |> dplyr::filter(contrast == "all")

p1_pAUC <- ggplot2::ggplot(resAllB, ggplot2::aes(x = Name, y = pAUC_10)) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::facet_wrap(~whatfix)  + 
  ggplot2::coord_cartesian(ylim = c(min(resAllB$pAUC_10),max(resAllB$pAUC_10))) + 
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = meanpAUC_10), color = "red") + ggplot2::xlab("")

p2_compare_variousLevels <- ggplot2::ggplot(res, ggplot2::aes(x = contrast, y = pAUC_10n, group = Name)) +
  ggplot2::geom_line(stat = "identity", ggplot2::aes(linetype = Name, color = Name), size = 1) + 
  ggplot2::facet_wrap(~ whatfix, scales = "free") +
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = 0), color = "red") + 
  ggplot2::theme(legend.position = "bottom", legend.title = ggplot2::element_blank())

```

A relevant parameter is the number of contracts a given method estimates from the data(see Figure \@ref(fig:FDRfdp) left Panel). It indicates how robust the procedure works in the presence of missing data. For each protein, we can estimate four contrast which for 4046 proteins, at least two peptides resulted in $16184$ possible contrast estimates. Because the set of contrasts slightly differs for each method, it makes comparing them complicated. For instance, we observe that `r BiocStyle::Biocpkg("MSstats")` estimates $16058$ while the mixed effect models estimate the fewest with $15940$. Hence, to conclude that one method shows a better performance, the score, e.g., pAUC, needs to be greater, but also the number of estimated contrasts needs to be equal or larger.

Figure \@ref(fig:FDRfdp) left panel, shows how various estimates obtained from the models, i.e. $\log_2(FC)$, $t$-statistics and scaled $p$-values allow to identify true positives (TP) given a false positive rate. When testing for differential expression, the $\log_2(FC)$ is outperformed by than the $t$-statistic or $p$-value. 
If an accurate estimate of the $log_2(FC)$ is essential, the linear models fitted to protein intensities, estimated using Tukey's median polish, show the best performance. We speculate that this is because of the robustness of Tukey's median polish method. This hypothesis could be examined, by including other forms of protein intensity inferences implemented in `prolfqua`, e.g. `top-N` or `rlm`, into the benchmark.

There are only minor differences in the $p$-AUC when using the $t$-statistics or the scaled $p$-value  (see Figure \@ref(fig:FDRfdp) right panel). In order to compute the confusion matrices based on the $p$-value we first need to rescale it (see Equation \@ref(eq:scalepvalue)). Thus, the $p$-value is a function of the $t$-statistics and the degrees of freedom. Interestingly, the pAUC increases when using $p$-values instead of the $t$-statistics for linear models, while it decreases for mixed effect models. The reason is erroneous denominator degrees of freedom estimation for many proteins in the mixed-effect models.

We also benchmark, if the $FDR$ obtained from a model is an unbiased estimate of the false discovery proportion $FDP$. Figure \@ref(fig:FDRfdp) right panel draws on the horizontal axis the FDR determined from the model, and on the vertical axis, the FDP obtained from the confusion matrix. We see that most of the lines are below the diagonal, indicating that the FDR estimates obtained with `r BiocStyle::Githubpkg("fgcz/prolfqua")` are modestly conservative for this benchmark dataset provide. In the case of `r BiocStyle::Biocpkg("MSstats")`, we observe a high proportion of false discoveries for small $FDR$ values. In the case of `r BiocStyle::Biocpkg("PECA")`, the FDR estimates, obtained when Benjamini-Hochberg correcting the regulation probabilities, strongly overestimate the $FDP$.

(ref:FDRfdp) Left panel - Partial area under the ROC curve at $10\%$ FPR for all contrasts and three different statistics: the difference among conditions, the scaled $p$-value (sign(diff) * p.value) and the $t$-statistics (higher is better). The red line average area under the curve. Left panel - Number and percentage of estimated contrasts for each modeling method (higher is better). Right panel - Compare FDR estimate with false discovery proportion (FDP) closer to the diagnoal (red continous line is better).


```{r FDRfdp, fig.cap = "(ref:FDRfdp)", out.width = '90%', fig.width=12, fig.height=10}
dd <- purrr::map_df(allBenchmarks, function(x){res <- x$smc$summary; res$name <- x$model_name;res})
dd <- dd |> dplyr::mutate(nrcontrasts = protein_Id * (4 - nr_missing))
dds <- dd |> dplyr::group_by(name) |> dplyr::summarize(nrcontrasts = sum(nrcontrasts))
dds$percent <- dds$nrcontrasts/max(dds$nrcontrasts) * 100

nrgg <- dds |> ggplot2::ggplot(ggplot2::aes(x = name, y = nrcontrasts )) + 
  ggplot2::geom_bar(stat = "identity", fill="white", colour = "black") + 
  ggplot2::coord_cartesian(ylim = c(min(dds$nrcontrasts) - 100, max(dds$nrcontrasts) + 10)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_text(ggplot2::aes(label = round(nrcontrasts, digits = 1)),
            vjust = 1, hjust = -0.2, angle = -90) + 
  ggplot2::geom_text(ggplot2::aes(label = paste0("(",round(percent, digits = 1),"%)")),
            vjust = -1, hjust = -0.2, angle = -90) 
p3 <- nrgg

dd <- purrr::map_df(allBenchmarks, function(x){res <- x$get_confusion_FDRvsFDP(); res$name <- x$model_name;res})

ddb <- dplyr::filter(dd, contrast == "dilution_(4.5/3)_1.5")
ddb <- dd |> dplyr::filter(contrast == "dilution_(7.5/6)_1.25")
ddb <- dd |> dplyr::filter(contrast == "all")


p4 <- ddb |> ggplot2::ggplot(ggplot2::aes(y = FDP_,  x  = scorecol )) + 
  ggplot2::geom_line(ggplot2::aes(color = model_name, linetype = model_name)) +
  ggplot2::facet_wrap(~contrast) + 
   ggplot2::geom_abline(intercept = 0, slope = 1, color = 2) + 
   ggplot2::theme(legend.position = "bottom")


hlay = rbind(c(1,1),
            c(2,3))


gridExtra::grid.arrange(p1_pAUC, p3, p4, layout_matrix=hlay)
```



# Results and Discussion.

We focused on comparing models to estimate fold changes, variances and to obtain false discovery rates. 

[TODO(wew): Goes in the discussion or introduction, and give references for normalization e.g. VSN paper.] However, an essential parameter, not discussed further, is the normalization of the intensities within the samples to make them comparable. For this benchmark dataset, the fraction of regulated proteins (_E. coli_) is relatively large. Therefore, to calibrate the measurement, we used human proteins only. In practice, it is unknown which proteins do and which don't change. Therefore, the assumption used when normalizing, scaling the data, and thus calibrating the measurement is that only a small subset of the proteins in the dataset is differentially regulated.

[TODO(wew): Goes to discussion] The method used to infer protein intensities from more peptide intensities is an additional important factor. The original _proDA_ publication uses MaxLFQ [@cox2014accurate] protein estimates. When using those, _proDA_ performs significantly worse than _proDA_ when using the protein estimates obtained from peptide intensity estimates using Tukey's median polish,  $pAUC_{10}(\textrm{t-statistics}) = 66%$ and $pAUC_{10}(\textrm{t-statistics}) = 73%$ respectively (see Articles on [@prolfquaGithubIO] for more details).
 

We assessed the performance of different modeling approaches implemented in prolfqua, `r BiocStyle::Biocpkg("MSstats")` and `r BiocStyle::Biocpkg("proDA")`, using a benchmark dataset whose ground truth is known. However, the dataset used has several limitations: first, the design is a parallel-group design, i.e., we could not benchmark how well interactions are modelled; secondly, essential sources of variation present in every experiment, such as the biochemical and biological, are not measured. The dataset captures only the variance from the chromatography, electro-spray, and mass spectrometric measurement method. Thus, while we can extrapolate some of the results obtained to more realistic datasets, we need to be careful not to over-interpret the results.

We can conclude that if we want to order the proteins according to the likelihood of being differentially regulated, for instance, to perform gene enrichment analysis, the $t$-statistic is better suited than the fold-change estimate. Modeling the degrees of freedom when computing the $p$-values might improve the inference further. However, the observed change is minuscule. 

For the fixed effect linear model, the empirical Bayes variance shrinkage, as suggested by [@Smyth2004linear], consistently improves compared with the unmoderated estimates (data not shown). However, there is no such effect for the mixed effect model, most likely because the degrees of freedom are erroneously estimated for a large proportion of models.

Computing the statistics at the peptide level, e.g., the $t$-statistics or $p$-value, then summarizing these statistics using their median produces the highest AUC scores among all the tested models. By using the beta distribution to model the number of peptides observed, we can further improve the pAUC scores (see Figure TODO(wew)). However, the properties of beta-based probabilities are not well understood; for instance, they have no uniform distribution under the null hypothesis (TODO remove they). The FDR estimates obtained when correcting multiple testing with the Benjamini-Hochberg method are strongly biased (see Figure \@ref(fig:FDRfdp) right panel). Therefore, we can not recommend this method if an unbiased estimate of FDR is essential, which is frequently the case. Furthermore, peptides are stronger affected by missing values, reducing the number of contrasts we could estimate for the dataset (see Figure \@ref(fig:FDRfdp) left panel). To address the issue of missing observation, we could either run `r BiocStyle::Biocpkg("proDA")` or the imputation of group averages method discussed here and afterwards apply `r BiocStyle::Biocpkg("PECA")`.

The probabilistic dropout analysis implemented in the `r BiocStyle::Biocpkg("proDA")` produces inferences comparable to other methods (Figure \@ref(fig:FDRfdp) for the largest number of proteins (Figure \@ref(fig:FDRfdp) left panel). Moreover, the estimated fold changes are unbiased and show high diagnostic accuracy. Furthermore, the scaled $p$-values and $t$-statistics are on par with those obtained by the linear model with variance moderation. Therefore, we plan to integrate the `r BiocStyle::Biocpkg("proDA")` package as an additional modeling option into `r BiocStyle::Githubpkg("fgcz/prolfqua")` (see Figure \@ref(fig:ContrastUML)).

Leveraging on these computer experiments, we can provide the following advice. First, aggregate the peptide intensities to protein intensities using a robust regression method. Second, use linear models as long as they suffice to model your experimental design. Use mixed effect models only if you have to, e.g., repeated measurements. If you use fixed-effect linear models, apply variance moderation since this improves the $t$-statistics and $p$-value estimates. If you want to sort your protein lists to perform gene set enrichment analysis, use the $t$-statistic instead of the fold change. You can select proteins to study them further based on the FDR, although expect a $bias$.

`r BiocStyle::Githubpkg("fgcz/prolfqua")` allows for considerable flexibility to model your experiments. Various types of models are available (for an overview see table TODO(wew) create table with methods implemented in prolfqua), and the contrast specification is consistent and intuitive for all models. The benchmarking results are comparable with existing methods. The modular design of `r BiocStyle::Githubpkg("fgcz/prolfqua")`, allows for customizing the analysis or adding new features, e.g., generalized linear models (_glm_'s) for missing data modeling or robust linear models (_rlm_'s), quickly.

Using the tidy table to model the data ensures interoperability with other proteomics-related packages that manage their data with tidy-tables, e.g., `r BiocStyle::CRANpkg("protti")`. Furthermore, the use of `R6` classes, which encapsulate the configuration and the data, allows writing very concise code (no function arguments needed), and auto-completion support in the Rstudio editor makes it easy for novices to explore `r BiocStyle::Githubpkg("fgcz/prolfqua")`s functionality. 

In summary `r BiocStyle::Githubpkg("fgcz/prolfqua")` is an easy-to-use and feature rich R package to analyse quantitative mass spectrometric data, report results, and benchmark MS software and statistical methods. We provide more details at the website https://github.com/fgcz/prolfqua/.

# Conclusion (tbd)


# Acknoledgements

The authors thank the technology platform fund (TPF) of the University of Zurich.

# Abbreviations

|       |                                                   |
|:------|:--------------------------------------------------|
|AUC    |Area Under the Curve                               |
|CDF    |Cumulative Distribution Function                   |
|ESI-MS |Electro-Spray-Ionization Mass Spectrometry         |
|FC     |Fold Change                                        |
|$FDP$  |False Discovery Proportion                         |
|$FDR$  |False Discovery Rate                               |
|HPLC   |High-Pressure Liquid Chromatography                |
|LC-MS  |Liquid Chromatography followed by Mass Spectrometry|
|LFQ    |Label-Free Quantification                          |
|LOD    |Limit Of Detection                                 |
|MS     |mass spectrometry                                  |
|OO     |Object-Oriented                                    |
|UML    |Unified Modeling Language                          |

# References {-}

<div id="refs"></div>

# Supplements

## Creating a prolfqa configuration

The following code snipet demomnstarte the usage from MSFragger ouput.

TODO(wew)

## Comparing benchmark results

Here we compare the benchmark results for the `r BiocStyle::Biocpkg("proDA")` model and the linear model with variance moderation, but using only the protein where inference could be performed for both models. We see that the fold change estimates obtained with the `r BiocStyle::Biocpkg("proDA")` package show better diagnostic accuracy.

```{r proDABenchmarks}
prodB <- allBenchmarks$benchmark_proDA$clone(deep=TRUE)
proda <- prodB$.data
modB <- allBenchmarks$benchmark_ProtModerated 

mod <- modB$.data
mod <- mod[!is.na(mod$statistic),]
modSum <- modB$pAUC_summaries()

prodaMod <- dplyr::semi_join(proda, mod, by = c("contrast", "protein_Id"))
prodB$.data <- prodaMod
d <- prodB$missing_contrasts()
prodSum <- prodB$pAUC_summaries()

tmp <- dplyr::inner_join(prodSum$ftable$content,
                         modSum$ftable$content,
                         by=c("contrast","what"), suffix = c(".proDA",".lmMod")) |>
  tidyr::unite("score_contr" , what, contrast, remove = FALSE)


ggplot2::ggplot(tmp, ggplot2::aes(x = contrast, y = pAUC_10.proDA - pAUC_10.lmMod)) + ggplot2::geom_bar(stat="identity") + ggplot2::facet_wrap(~what) +  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r proDAAllBenchmarks}
proda <- allBenchmarks$benchmark_proDA$data()
imp <- allBenchmarks$benchmark_imputation$data()

x <- dplyr::inner_join(imp, proda, by = c("contrast","protein_Id", "TP", "species"), suffix = c(".imp",".proda"))
ggplot2::ggplot(x,ggplot2::aes( x = diff.imp, y = diff.proda) ) + ggplot2::geom_point() + ggplot2::facet_wrap(~contrast + species) + ggplot2::geom_hline(yintercept = 0, color="red") + ggplot2::geom_vline(xintercept = 0, color="red")
```


## Session information

```{r sessioninfo, echo=FALSE}
sessionInfo()
```

## Package sticker

```{r proLFQuaSticker, echo=TRUE, out.height="5cm", eval=TRUE, fig.cap="Sticker maintainer: Witold E. Wolski; License: Creative Commons Attribution CC-BY. Feel free to share and adapt, but don't forget to credit the author."}

file.path(system.file(package = "prolfqua"), 
          "Figures/hexStickerProlfqua.png") |>
  knitr::include_graphics()
```




