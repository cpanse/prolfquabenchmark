---
title: "Prolfqua - Proteomics label free quantification using linear models."
author: 'Witold Wolski Std Nr:'
date: "6/11/2021"
output: 
  bookdown::html_document
bibliography: bibliography.bib

---


# Abstract


# Introduction

```{r setup, include=FALSE}
library(tidyverse)
library(prolfqua)
library(ggplot2)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

  The R-package for **pro**teomics **l**abel **f**ree **qua**ntification `prolfqua` (read: prolewka) evolved from functions and code snippets used to visualize and analyze label-free quantification data. To compute protein fold changes among treatment conditions, we first used t-test, linear models, or functions implemented in the package limma. We evaluated [MSStats]( 10.18129/B9.bioc.MSstats), [ROPECA](10.1038/s41598-017-05949-y) or [MSqRob](https://github.com/statOmics/MSqRob) all implemented in R, with the idea to integrate the various approaches in our analysis pipeline. Although these packages are implemented in R, model specification, input, and output formats differ widely and wildly. Furthermore, to better understand the inferences produced, we decided to use the R linear and mixed effect models directly and transparently and to reimplement some features from the other packages when considered useful. The R-package prolfqua is the outcome of this venture.

The _prolfqua_ package integrates most of the steps of the LFQ data analysis workflow: quality control, data normalization, protein aggregation, sample size estimation, modeling, and hypothesis testing. For instance, the quality control module makes it easy to determine the coefficients of variations (CV) for peptides and proteins within conditions, summarize and visualize missing data or intensities.

When developing _prolfqua_, we draw inspiration from packages such as _sf_, which uses data in a long tidy table format. We represent the data needed for analysis using a single data-frame in long format and an R6 configuration object. Using the configuration object, we specify what data is in which column, making it easy to integrate new inputs if provided in long tidy tables. All that is needed to incorporate Spectronaut, Skyline text output, or the __MSStats__ data format is a few code lines of code to update the configuration object. For software like MaxQuant or MSFragger, which writes the data into a wide table, with several intensity columns, one for each sample, we implemented methods that transform the data into a long tidy table. Relying on the long tidy data table enabled us to easily interface with many data manipulation, visualization, and modeling methods implemented in base R and the _tidyverse_.

 R linear model and linear mixed effect models allow modeling parallel designs, repeated measurements, factorial designs, and many more.  R's formula interface for linear models is flexible, widely used, and well documented. We integrate R's linear model and mixed model formula interface in _prolfqua_'s API. This glass box approach should make it easy to reimplement an analysis performed with prolfqua using base R or other programming languages by reading the analysis script and without looking at the package code. Knowledge of the R regression model infrastructure is an advantage when using our package. Acknowledging the formula interface's complexity and the popularity of MSstats, we provided functionality to derive the model formula from an MSstats data format.
 
The prolfqua package fits the linear model to all the proteins in the dataset. Afterward, we can compute contrast to test the hypothesis, perform ANOVA, or model selection using the likelihood ratio test for thousands of proteins. By exploiting the parallel structure of the analysis, _prolfqua_ implements t-statistic moderation to improve inference for small sample sizes [@Smyth2004linear]. It also computes probabilities of differential protein regulation based on peptide level models [@Suomi2017bEnhanced]. 

To optimize a data analysis workflow, prolfqua implements and integrates benchmarking methods, e.g., computing ROC curves. We implemented these methods to assess differences between linear models, mixed effect models, p-value moderation, or Bayesian regression models using the IonStar dataset.

We developed and improved the package by applying the "Eating your own dog food" principle. We used it to analyze SRM, PRM, and LFQ data and to generate analysis reports. The current iteration of the packages API groups the functionality into R6 classes (e.g., LFQPlotter, LFQSummarizer, LFQStats, Model, Contrast, etc.). We choose R6 for its visual debugging and code completion support in RStudio.

We believe that the package makes it easy to perform proteomics quantitative data analysis, generate visualizations, and integrate them into reproducible reports using Rmarkdown. You can install the package from [www.github.com/wolski/prolfqua](www.github.com/wolski/prolfqua), and we are working on making it available on Bioconductor.

# Methods

## Benchmarking

The benchmark dataset contains _H. sapiens_ proteins with constant concentrations and _E. coli_ proteins with varying concentration [@shen2018ionstar]. We know that for _H. sapiens_ proteins the difference $\beta$ between two dilutions should be $\beta = 0$ while for _E. coli_ proteins we know that the difference between dilutions should be $\beta \ne 0$.

We can use various statistics to examine this hypothesis: the contrast estimate $\beta$, or because we first log2 transformed the data the log2 fold-change (log2FC), the t-statistic $\frac{\beta}{\sqrt{var(\beta)}}$, the $p-value$  and moderated $p-value$ for models where the p-value can be obtained. 

For Bayesian models we can use the posterior probabilities that $\beta=0$, e.g. the probability that $\beta < 0$ when the mean estimate $\hat{\beta}$ is greater $0$ or vice versa, i.e.  $P(\beta < 0| \hat{\beta} > 0)$ or $P(\beta > 0 | \hat{\beta} < 0)$, which we can compute from the posterior sample of the $\beta$ parameters. We can also use the statistic $\frac{mean(\beta)}{\sqrt{var(\beta)}}$ with $var{\beta}$ estimated from the posterior sample of the $\beta$ parameter.

For each statistic and each value of the statistics we then compute a confusion matrix (see Table \@ref(tab:confusionMatrix)).
From the confusion matrix we obtain measures such as true positive rate (TPR), false positive rate (FPR), false discovery proportion (FDP) or accuracy (ACC) which are given by:


```{r confusionMatrix}
table <- data.frame( c("beta != 0", "beta == 0", "Total"), matrix(c("TP","FP","R","FN","TN","","P","N","m"), ncol = 3, byrow = T))
colnames(table) <- c("Prediction \\ Truth","E.coli", "H.sapiens", "Total")
knitr::kable(table, caption = "Confusion matrix, TP - true positive, FP - false positive, FN - false negative, P - all positive cases (all E. coli proteins), N - all negative cases (all H. sapiens proteins), m- all proteins.")

```



$$
\begin{aligned}
TPR &= \frac{TP}{TP+FN} = \frac{TP}{P}\\
FPR &= \frac{FP}{FP+TN} = \frac{FP}{N}\\
FDP &= \frac{FP}{TP + FP} = \frac{FP}{R}\\
ACC &= \frac{TP + TN}{m}
\end{aligned}
$$


By plotting the $TPR$ versus the $FPR$ we obtain the receiver operator characteristic curve (ROC curve). The area under the curve (AUC) or partial areas under the curve (pAUC), at various values of the $FPR$, are further measures of performance. By using these measures we can compare the performances of the statistics produced by the various methods examined.


A further question we can examine using the benchmark data is, how well the false discovery estimate (FDR) obtained to from the statistical model matches the false discovery proportion (FDP). The FDR is the expected value of the false discovery proportion. Ideally, the FDR should be an unbiased estimate of the $FDP$. By plotting the FDR gainst the FDP we can asses visually if these assumptions are met. 

For the Bayesian models we obtain an FDR estimate by first computing the p-value using the statistic $\frac{mean(\beta)}{\sqrt{var(\beta)}}$ and assuming that it is normally distributed. We then obtain the FDR by adjusting the  p-values using the Benjamini-Hochberg procedure.


For some of the proteins, quantification results are missing in some conditions. All we required when attempting to fit a model for a protein is that there are at least two peptides per protein quantified in any of the twenty samples. That means that for some proteins, not all model parameters are estimable. 
In case of the linear model, the contrast function we implemented can handle partial models and provides estimates of some of the contrasts. For the mixed effect models, the used `lmerTest::contest` function does not support models with missing parameter estimates, and hence we have either four or zero contrast estimates for a protein. The ROPECA approach starts by fitting a model to each peptide and the contrast function can handle missing parameter estimates.
These differences result in different numbers of fold-change estimates for each type of model, i.e. in a different set of ground truths, which makes comparing model performances involved.


## Benchmark Data preprocessing


We fitted the models using all the $20$ samples of the IonStar dataset (see Figure \@ref(fig:ionstar)), i.e. five different dilutions, with four technical replicates each. This allows to estimate ten different contrasts shown in Table \@ref(tab:setupContrasts). For benchmarking we only used the contrasts resulting in small fold-changes $\beta = 1.2,1.25,1.3(3),1.5$ listed in Table \@ref(tab:usedContrasts). Only those small fold-changes allowed us to see differences among the methods. For the larger fold-changes no differences can be seen among the models or statistics.

Data was preprocessed by $log_2$ transforming the peptide intensities, and subsequent robust z-score transformation. No other data filtering was used than the removal of _one hit wonders_, i.e. proteins with a single peptide assignment (For details see section [Datasets and Data preprocessing]). We did not use imputation.  The only differences we will observe are because of using different modelling methods.


$$
z = \frac{x - \bar{x}}{S} \sim \frac{x - \tilde{x}}{\tilde{S}}
$$  t fold change on the original scale, we multiply the z-score by the average variance of all the $N$ samples in the experiment.

$$
z' = z \times 1/N\sum_{i=1}^N S_i
$$
We decided to apply this simple transformation because it only requires to estimate two parameters per sample. For the ionstar dataset we estimated \bar{x} and $S$ using the human proteins only.




```{r setupContrasts}

outpath <- "results_modelling_all"
#modelName  <- "Model"


DEBUG <- FALSE

Contrasts <- c(
  "dilution_(9/3)_3" =   "dilution.e - dilution.a",
  "dilution_(9/4.5)_2" =   "dilution.e - dilution.b",
  "dilution_(9/6)_1.5" =   "dilution.e - dilution.c",
  "dilution_(9/7.5)_1.2" =   "dilution.e - dilution.d",
  
  "dilution_(7.5/3)_2.5" =   "dilution.d - dilution.a",
  "dilution_(7.5/4.5)_1.6(6)" =   "dilution.d - dilution.b",
  "dilution_(7.5/6)_1.25" =   "dilution.d - dilution.c",
  
  "dilution_(6/3)_2" =   "dilution.c - dilution.a",
  "dilution_(6/4.5)_1.3(3)" =   "dilution.c - dilution.b",
  
  "dilution_(4.5/3)_1.5" =   "dilution.b - dilution.a"
)

tt <- data.frame(contrastsName = names(Contrasts),  contrasts = Contrasts)
tt <- tt %>% separate(contrastsName, c("dilution", "ratio", "expected fold change"), sep =  "_", remove = FALSE)

```



```{r usedContrasts}
relevantContrasts <- c("dilution_(9/7.5)_1.2","dilution_(7.5/6)_1.25", "dilution_(6/4.5)_1.3(3)", "dilution_(4.5/3)_1.5" )

tt <- tt %>% dplyr::filter(contrastsName  %in% relevantContrasts)
tt <- dplyr::select(tt, all_of(c("contrasts","ratio", "expected fold change")))
knitr::kable(tt, caption = "Contrasts used for benchmark.")


```

Furthermore, we will only model protein fold-changes of proteins were at least two peptides were observed. The reason is that peptide identification has an associated error which is significantly reduced if two independent peptide identifications of a protein are observed. Many proteomics journals require to only report proteins identified by two peptides. Proteins identified by only a single peptide are sometimes called "one-hit wonders" [@mendoza2018flexible].


In order to remove systematic differences among samples, peptide intensities need to be transformed and scaled. The transformation aims to remove heteroscedasticity while the scaling aims to remove systematic differences among samples. Because the size of the error is proportional to the intensity, by $\log$ transforming the intensities this error can be modelled with $\epsilon \propto N(0, \sigma^2)$. 


Valikangas [@valikangas2016systematic] and colleagues discuss and benchmark various methods of peptide or protein intensity normalization such as variance stabilizing normalization [@huber2002variance] or quantile normalization [@bolstad2003comparison]. In this work we will be using a robust version of the z-score, where instead of the mean we use the median and instead of the standard deviation the median absolute deviation (mad):

$$
z = \frac{x - \bar{x}}{S} \sim \frac{x - \tilde{x}}{\tilde{S}}
$$  

Because we need to estimate the protein fold-changes on the original scale, we have to multiply the $z$-score by the average variance of all the $N$ samples in the experiment.

$$
z' = z \times 1/N\sum_{i=1}^N S_i
$$
We decided to apply this simple transformation because it only requires to estimate two parameters per sample and works for experiments with thousands of proteins as well as for experiments where only a few hundreds of proteins per sample are measured.
For the Ionstar dataset we used the intensities of _H. sapiens_ proteins, whose concentrations do not change, to determine $\bar{x}$ and $S$ and than applied it to all the intensities (including _E. coli_) in the sample.

Figure \@ref(fig:scaling) shows the distribution of the peptide intensities within the samples before and after the intensity scaling. Figure \@ref(fig:variancestable) shows the coefficient of variation before log transforming and scaling the data and the distribution of the standard deviations afterwards.


(ref:scaling) Density plot of peptide intensities before (panel A) and after (panel B) data transformation and scaling.

```{r scaling, fig.cap="(ref:scaling)", fig.width=7, fig.height=7}
is <- prolfqua::data_ionstar
pepsum <- is$filtered()
summarised <- is$subset_normalized()

p1 <-
  prolfqua::plot_intensity_distribution_density(pepsum$data,
                                                  pepsum$config) +
  theme(legend.position = "none") +
  labs(tag = "A") + theme(legend.position = "none")
p2 <-
  prolfqua::plot_intensity_distribution_density(
    summarised$data,
    summarised$config) +
  labs(tag = "B") + theme(legend.position = "none")
rocp <-
      ggpubr::ggarrange(
        p1,
        p2,
        nrow = 1,
        common.legend = TRUE,
        legend = "none"
      )
rocp
```


## Linear models 

We fitted a linear model implemented by the R function `lm` to protein intensities inferred from peptide intensities using the Tukey's median polish. The second mixed-effects model, implemented in the R function `lmer`, fitted to peptide level intensities. We modeled the peptide measurements as repeated measurements of the protein. The third model is again a linear model but this time fitted to peptide intensities. By this we have for each protein several models, which we then summarize. 

## Contrast estimation with imputation

To handle cases, where no observations were made in one of the conditions, we model the condition estimates and variances as follows. Either we use the mean of the protein intensity in the group, or if there are no observations we impute by using the average of $10\%$ smallest group mean intensities in the experiment. Here we assume that if there are no observations it is because of the limit of detection (LOD). To estimate the variance, we compute the pooled variance and standard deviation of all conditions $\hat{var}$. We assume that the variance of the protein is constant in all the conditions. The t-statistic is then given by $\frac{\bar x}{\hat{\sigma}}$.

## Computing contrasts

Given a linear model contrasts can be computed by $\hat{\beta_{c}} = \sum l\beta_{m}$ and $\textrm{var} \hat\beta_c = l\sigma^2 (X^T X)^{-1} l^T$, with $X$ being the design matrix, $\beta_m$ the model parameters and $l$ an array of coefficients. The degrees of freedom for the contrast are equal to the residual degrees of freedom of the linear model.
For estimating contrasts from mixed effects models we used the function `contest` implemented in the R package `lmerTest` [@Kuznetsova2017lmerTest] and used the Satterthwaite method to estimate the denominator degrees of freedom.

## P-value moderation

From the linear and the mixed effect models, we can obtain the residual standard deviation $\sigma$, and degrees of freedom.  [@Smyth2004linear] discuss how, using the empirical Bayes paradigm, to use the $\sigma$ and $df$ of all models to estimate a prior $\sigma$ and prior degrees of freedom, and posterior $\tilde \sigma$. These can be used to moderate the t-statistics by:
$\tilde{t}_{pj} = \frac{t_{pj} s_p}{\tilde{s}_p}$
and p-values.

## Summarizing peptide level models

To summarize peptide level models to protein models we did applied the method suggested by [@Suomi2017bEnhanced], that is to use the median scaled p-value of the peptide models and cumulative distribution function of the beta distribution function to determine a regulation probability of the protein. 

To obtain the median p-value of the protein, we first rescaled the peptide p-values by taking the direction of the fold-change $\hat \beta$ into account, i.e.:


\begin{equation}
p_{s} =
  \begin{cases}
1-p, & \textrm{if}~ \hat{\beta} > 0\\
p-1, & \textrm{otherwise}
\end{cases}
\end{equation}

Afterwards, the median scaled p-value $\tilde{p}_s$ is determined and using the transformation below, transformed back onto the original scale. 

$$
\tilde{p} = 1 - |\tilde{p}_{s}|
$$
Because we used the median as the i-th order statistic  $i = n/2 +0.5$. Therefore, $\gamma = i = n/2 + 0.5$ and $\delta = n - i + 1 = n - (n/2 + 0.5) + 1 = n/2 + 0.5 = \gamma$ are used to parameterize the CDF of the Beta distribution.



# Results and discussion

## Method comparison

In this section we compare the results for the various models implemented and discussed previously, namely:

* (`prot_med_lm`) : linear model fitted to protein estimates obtained using Tukey's median polish 
* (`prot_lme4`) : mixed effects model fitted using _lme4_ to peptide intensities.
* (`prot_ROPECA`) : linear models fitted on peptide level and aggregation of moderated p-values using the beta distribution

The table below summarizes the contrast estimates produced which will be benchmarked.

|                    | Model                   | Contrast   | Moderation   | Aggregation  |
|--------------------|:-----------------------:|------------|--------------|--------------|
| Protein Intensity  |  lm                     | o          |  o           |              |
| Protein Intensity Imputed | pooled variance  | o          |  o           |              |
| Peptide Intensity  |  lmer                   | o          |  o           |              |
| Peptide Intensity  |  lm                     |            |              |  o           |


A relevant parameter is the number of proteins for which we estimated the contrasts (see Table \@ref(tab:completeCasesTab)). It indicates how robust the models are in the presence of missing data. We observe, that when using the linear model on protein level intensities or the bayesian version of the mixed effects models we were able to estimate for 4024 (out of the 4099) proteins all the four examined contrasts (complete cases). When using the mixed effect model implementation in _lme4_ this number drops to $4013$, while when summarizing peptide level models it drops further to $3999$ proteins. The different number of proteins for which we obtained statistics of contrasts makes comparing the scores for various models complicated because the set of proteins differs, e.g. $4024$ versus $3999$. 
Therefore, we incorporate the failure to generate statistics for contrasts into the benchmark by setting $P$ and $N$ equal to the number of all proteins (see Table \@ref(tab:confusionMatrix)) and then computing the $FPR$ and $TPR$.



```{r}
allBenchmarks <- readRDS("../../inst/Benchresults/allBenchmarks.RDS")
benchmark_msstats <- readRDS("../../inst/Benchresults/benchmark_msstats.RDS")
msFragger <- readRDS(file = "../../inst/Benchresults/MSFragger_medpol_benchmark.RDS")

allBenchmarks$benchmark_mssstats <- benchmark_msstats
allBenchmarks$benchmark_msFragger <- msFragger

names(allBenchmarks)
allBenchmarks <- allBenchmarks[c("benchmark_imputation","benchmark_ProtModerated",  "benchmark_mixedModerated", "benchmark_ropeca","benchmark_merged","benchmark_mssstats", "benchmark_msFragger"   )]
```


```{r  fig.cap="Number and percentage of estimated contrasts by modelling method.", fig.width=5, fig.height=7}

dd <- map_df(allBenchmarks, function(x){res <- x$smc$summary; res$name <- x$model_name;res})
dd <- dd %>% mutate(nrcontrasts = protein_Id * (4 - nr_missing))
dds <- dd %>% group_by(name) %>% summarize(nrcontrasts = sum(nrcontrasts))
dds$percent <- dds$nrcontrasts/max(dds$nrcontrasts) * 100

nrgg <- dds %>% ggplot(aes(x = name, y = nrcontrasts )) + 
  geom_bar(stat = "identity", fill="white", colour = "black") + 
  coord_cartesian(ylim = c(min(dds$nrcontrasts) - 100, max(dds$nrcontrasts) + 10)) +
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) +
  geom_text(aes(label = round(nrcontrasts, digits = 1)),
            vjust = 1, hjust = -0.2, angle = -90) + 
  geom_text(aes(label = paste0("(",round(percent, digits = 1),"%)")),
            vjust = -1, hjust = -0.2, angle = -90) 
nrgg

```


```{r fig.cap="Partial area under the ROC curve at 10% FPR. Red line average area under the curve.", fig.width=10, fig.width=10}

ttt <- sapply(allBenchmarks, function(x){x$complete(FALSE)})
res <- map_df(allBenchmarks, function(x){x$pAUC()})
res <- res %>% mutate(whatfix = case_when(what == "scaled.beta.based.significance" ~ "scaled.p.value", TRUE ~ what))

res$contrast %>% unique()
#resAllB <- res %>% dplyr::filter(contrast == "dilution_(9/7.5)_1.2")
#resAllB <- res %>% dplyr::filter(contrast == "dilution_(4.5/3)_1.5")
#resAllB <- res %>% dplyr::filter(contrast == "dilution_(7.5/6)_1.25")



norm <- res %>% group_by(contrast,whatfix) %>% summarize(meanpAUC_10 = mean(pAUC_10))
res <- inner_join(res, norm)
res <- mutate(res , pAUC_10n = pAUC_10 - meanpAUC_10)

resAllB <- res %>% dplyr::filter(contrast == "all")

p1 <- ggplot(resAllB, aes(x = Name, y = pAUC_10)) +
  geom_bar(stat = "identity") +
  facet_wrap(~whatfix)  + 
  coord_cartesian(ylim = c(min(resAllB$pAUC_10),max(resAllB$pAUC_10))) + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) +
  geom_hline(aes(yintercept = meanpAUC_10), color="red")
p1
```


```{r fig.cap="Difference to mean partial area under the ROC curve for various models, at 10% FPR, by fold change.", fig.width=10, fig.width=10}


p2 <- ggplot(res, aes(x = contrast, y = pAUC_10n, group = Name)) +
  geom_line(stat = "identity",aes(linetype = Name, color = Name)) + 
  facet_wrap(~whatfix, scales = "free") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5)) +
  geom_hline(aes(yintercept = 0), color = "red")

p2
```



```{r FDRfdp, fig.cap = "Compare FDR estimate with false discovery proportion (FDP).", fig.width=8, fig.height=8}
dd <- map_df(allBenchmarks, function(x){res <- x$get_confusion_FDRvsFDP(); res$name <- x$model_name;res})
ddb <- filter(dd, contrast == "dilution_(4.5/3)_1.5")
ddb <- dd %>% dplyr::filter(contrast == "dilution_(7.5/6)_1.25")
ddb <- dd %>% dplyr::filter(contrast == "all")


ddb %>% ggplot(aes(y = FDP_,  x  = scorecol )) + 
  geom_line(aes(color = model_name, linetype = model_name)) +
  facet_wrap(~contrast) + 
   geom_abline(intercept = 0, slope = 1, color = 2)

```



```{r fig.cap="Distribution of the FDR values produced."}
dd %>% ggplot(aes(x = scorecol)) + geom_histogram() + facet_wrap(~model_name)
```


\begin{acknowledgement}

The authors thank the technology platform fund of the University of Zurich.


\end{acknowledgement}

\begin{suppinfo}

This will usually read something like: ``Experimental procedures and
characterization data for all new compounds. The class will
automatically add a sentence pointing to the information on-line:

\end{suppinfo}

## References



